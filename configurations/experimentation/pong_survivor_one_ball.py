from configurations.structure.experimentation_configuration import ExperimentationConfiguration
from environments.pong_survivor.configurations import classic_one_ball
from rllib.architectures.dense import Dense

pong_survivor_one_ball = ExperimentationConfiguration(
    experimentation_name='pong_survivor_one_ball',
    environment_name='PongSurvivor',
)
pong_survivor_one_ball.environment_configuration = classic_one_ball

# Ray
pong_survivor_one_ball.ray_local_mode = False

# Reinforcement Learning
# pong_survivor_one_ball.reinforcement_learning_configuration.architecture = Dense
pong_survivor_one_ball.reinforcement_learning_configuration.number_environment_runners = 15
pong_survivor_one_ball.reinforcement_learning_configuration.number_gpus_per_learner = 1
pong_survivor_one_ball.reinforcement_learning_configuration.train_batch_size = 40_000
pong_survivor_one_ball.reinforcement_learning_configuration.minibatch_size = 10_000

# Trajectory Dataset Generation
pong_survivor_one_ball.trajectory_dataset_generation_configuration.number_environment_runners = 10
pong_survivor_one_ball.trajectory_dataset_generation_configuration.number_gpus_per_environment_runners = 1/10
pong_survivor_one_ball.trajectory_dataset_generation_configuration.number_iterations = 200
pong_survivor_one_ball.trajectory_dataset_generation_configuration.minimal_steps_per_iteration_per_environment_runners = 10_000

# pong_survivor_one_ball.trajectory_dataset_generation_configuration.save_rendering = True
# pong_survivor_one_ball.trajectory_dataset_generation_configuration.number_environment_runners = 5
# pong_survivor_one_ball.trajectory_dataset_generation_configuration.number_gpus_per_environment_runners = 1/5
# pong_survivor_one_ball.trajectory_dataset_generation_configuration.number_iterations = 20
# pong_survivor_one_ball.trajectory_dataset_generation_configuration.minimal_steps_per_iteration = 1000

# Surrogate Policy Training
# pong_survivor_one_ball.surrogate_policy_training_configuration.clusterization_loss = None
pong_survivor_one_ball.surrogate_policy_training_configuration.batch_size = 20_000
pong_survivor_one_ball.surrogate_policy_training_configuration.mini_chunk_size = 100_000
pong_survivor_one_ball.surrogate_policy_training_configuration.number_mini_chunks = 2
pong_survivor_one_ball.surrogate_policy_training_configuration.clusterization_loss_coefficient = 1.0
pong_survivor_one_ball.surrogate_policy_training_configuration.clusterization_loss_configuration.update({
    'number_cluster': 4,
})

