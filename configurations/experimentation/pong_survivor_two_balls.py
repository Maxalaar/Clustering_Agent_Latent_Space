from configurations.structure.experimentation_configuration import ExperimentationConfiguration
from environments.pong_survivor.configurations import classic_two_balls
from rllib_repertory.architectures.dense_ppo import DensePPO

pong_survivor_two_balls = ExperimentationConfiguration(
    experimentation_name='pong_survivor_two_balls',
    environment_name='PongSurvivor',
)
pong_survivor_two_balls.environment_configuration = classic_two_balls

# Ray
pong_survivor_two_balls.ray_local_mode = False

# Reinforcement Learning
pong_survivor_two_balls.reinforcement_learning_configuration.training_name = 'base'
pong_survivor_two_balls.reinforcement_learning_configuration.architecture = DensePPO
pong_survivor_two_balls.reinforcement_learning_configuration.number_environment_runners = 16
pong_survivor_two_balls.reinforcement_learning_configuration.number_environment_per_environment_runners = 1
pong_survivor_two_balls.reinforcement_learning_configuration.number_gpus_per_learner = 1
pong_survivor_two_balls.reinforcement_learning_configuration.train_batch_size = 40_000
pong_survivor_two_balls.reinforcement_learning_configuration.minibatch_size = 10_000
pong_survivor_two_balls.reinforcement_learning_configuration.batch_mode = 'complete_episodes'
pong_survivor_two_balls.reinforcement_learning_configuration.number_epochs = 16
pong_survivor_two_balls.reinforcement_learning_configuration.clip_policy_parameter = 0.1

# Video Episodes
pong_survivor_two_balls.video_episodes_generation_configuration.number_environment_runners = 5

# Trajectory Dataset Generation
pong_survivor_two_balls.trajectory_dataset_generation_configuration.number_environment_runners = 10
pong_survivor_two_balls.trajectory_dataset_generation_configuration.number_iterations = 100
pong_survivor_two_balls.trajectory_dataset_generation_configuration.minimal_steps_per_iteration_per_environment_runners = 1000

# Rendering Trajectory Dataset Generation
pong_survivor_two_balls.rendering_trajectory_dataset_generation_configuration.number_environment_runners = 5
pong_survivor_two_balls.rendering_trajectory_dataset_generation_configuration.number_iterations = 10
pong_survivor_two_balls.rendering_trajectory_dataset_generation_configuration.minimal_steps_per_iteration_per_environment_runners = 100

# Surrogate Policy Training
pong_survivor_two_balls.surrogate_policy_training_configuration.training_name = '4_cluster_1_repulsion'
pong_survivor_two_balls.surrogate_policy_training_configuration.clusterization_function_configuration.update({
    'number_cluster': 4,
})
pong_survivor_two_balls.surrogate_policy_training_configuration.clusterization_loss_configuration.update({
    'number_centroids_repulsion': 1,
})

# Surrogate Policy Evaluation
pong_survivor_two_balls.surrogate_policy_evaluation_configuration.evaluation_duration = 1_000
pong_survivor_two_balls.surrogate_policy_evaluation_configuration.number_environment_runners = 10
pong_survivor_two_balls.surrogate_policy_evaluation_configuration.number_gpus_per_environment_runners = 0.1